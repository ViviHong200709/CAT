{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CAT\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setuplogger():\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"[%(levelname)s %(asctime)s] %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    root.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "setuplogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose dataset here\n",
    "dataset = 'junyi'\n",
    "# modify config here\n",
    "# irt\n",
    "config = {\n",
    "    'learning_rate': 0.0025,\n",
    "    'batch_size': 2048,\n",
    "    'num_epochs': 6,\n",
    "    'num_dim': 1, # for IRT or MIRT\n",
    "    'device': 'cuda:0',\n",
    "    # for NeuralCD\n",
    "    'prednet_len1': 128,\n",
    "    'prednet_len2': 64,\n",
    "}\n",
    "# #mirt\n",
    "# config = {\n",
    "#     'learning_rate': 0.002,\n",
    "#     'batch_size': 2048,\n",
    "#     'num_epochs': 10,\n",
    "#     'num_dim': 10, # for IRT or MIRT\n",
    "#     'device': 'cpu',\n",
    "#     # for NeuralCD\n",
    "#     'prednet_len1': 128,\n",
    "#     'prednet_len2': 64,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "train_triplets = pd.read_csv(f'../data/{dataset}/train_triples.csv', encoding='utf-8').to_records(index=False)\n",
    "concept_map = json.load(open(f'../data/{dataset}/concept_map.json', 'r'))\n",
    "concept_map = {int(k):v for k,v in concept_map.items()}\n",
    "metadata = json.load(open(f'../data/{dataset}/metadata.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CAT.dataset.TrainDataset(train_triplets, concept_map,\n",
    "                                      metadata['num_train_students'], \n",
    "                                      metadata['num_questions'], \n",
    "                                      metadata['num_concepts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 2022-06-13 16:18:03,352] train on cuda:0\n",
      "[INFO 2022-06-13 16:18:03,539] Epoch [1] Batch [0]: loss=inf\n",
      "[INFO 2022-06-13 16:18:05,162] Epoch [1] Batch [20]: loss=0.87573\n",
      "[INFO 2022-06-13 16:18:06,884] Epoch [1] Batch [40]: loss=0.75021\n",
      "[INFO 2022-06-13 16:18:08,607] Epoch [1] Batch [60]: loss=0.68374\n",
      "[INFO 2022-06-13 16:18:10,194] Epoch [1] Batch [80]: loss=0.64783\n",
      "[INFO 2022-06-13 16:18:11,870] Epoch [1] Batch [100]: loss=0.62631\n",
      "[INFO 2022-06-13 16:18:13,595] Epoch [1] Batch [120]: loss=0.61272\n",
      "[INFO 2022-06-13 16:18:15,200] Epoch [1] Batch [140]: loss=0.60218\n",
      "[INFO 2022-06-13 16:18:16,880] Epoch [1] Batch [160]: loss=0.59401\n",
      "[INFO 2022-06-13 16:18:18,546] Epoch [1] Batch [180]: loss=0.58797\n",
      "[INFO 2022-06-13 16:18:20,319] Epoch [1] Batch [200]: loss=0.58246\n",
      "[INFO 2022-06-13 16:18:21,993] Epoch [1] Batch [220]: loss=0.57804\n",
      "[INFO 2022-06-13 16:18:23,777] Epoch [1] Batch [240]: loss=0.57402\n",
      "[INFO 2022-06-13 16:18:25,368] Epoch [1] Batch [260]: loss=0.57019\n",
      "[INFO 2022-06-13 16:18:27,038] Epoch [1] Batch [280]: loss=0.56695\n",
      "[INFO 2022-06-13 16:18:28,704] Epoch [1] Batch [300]: loss=0.56391\n",
      "[INFO 2022-06-13 16:18:30,290] Epoch [1] Batch [320]: loss=0.56124\n",
      "[INFO 2022-06-13 16:18:31,951] Epoch [1] Batch [340]: loss=0.55837\n",
      "[INFO 2022-06-13 16:18:33,652] Epoch [1] Batch [360]: loss=0.55567\n",
      "[INFO 2022-06-13 16:18:35,341] Epoch [1] Batch [380]: loss=0.55322\n",
      "[INFO 2022-06-13 16:18:35,973] Epoch [2] Batch [0]: loss=inf\n",
      "[INFO 2022-06-13 16:18:37,648] Epoch [2] Batch [20]: loss=0.52424\n",
      "[INFO 2022-06-13 16:18:39,325] Epoch [2] Batch [40]: loss=0.51389\n",
      "[INFO 2022-06-13 16:18:40,913] Epoch [2] Batch [60]: loss=0.50629\n",
      "[INFO 2022-06-13 16:18:42,576] Epoch [2] Batch [80]: loss=0.50114\n",
      "[INFO 2022-06-13 16:18:44,235] Epoch [2] Batch [100]: loss=0.49754\n",
      "[INFO 2022-06-13 16:18:45,811] Epoch [2] Batch [120]: loss=0.49526\n",
      "[INFO 2022-06-13 16:18:47,465] Epoch [2] Batch [140]: loss=0.49206\n",
      "[INFO 2022-06-13 16:18:49,122] Epoch [2] Batch [160]: loss=0.48971\n",
      "[INFO 2022-06-13 16:18:50,700] Epoch [2] Batch [180]: loss=0.48753\n",
      "[INFO 2022-06-13 16:18:52,345] Epoch [2] Batch [200]: loss=0.48503\n",
      "[INFO 2022-06-13 16:18:54,011] Epoch [2] Batch [220]: loss=0.48318\n",
      "[INFO 2022-06-13 16:18:55,581] Epoch [2] Batch [240]: loss=0.48163\n",
      "[INFO 2022-06-13 16:18:57,399] Epoch [2] Batch [260]: loss=0.47992\n",
      "[INFO 2022-06-13 16:18:59,048] Epoch [2] Batch [280]: loss=0.47860\n",
      "[INFO 2022-06-13 16:19:00,631] Epoch [2] Batch [300]: loss=0.47741\n",
      "[INFO 2022-06-13 16:19:02,305] Epoch [2] Batch [320]: loss=0.47599\n",
      "[INFO 2022-06-13 16:19:03,964] Epoch [2] Batch [340]: loss=0.47483\n",
      "[INFO 2022-06-13 16:19:05,555] Epoch [2] Batch [360]: loss=0.47363\n",
      "[INFO 2022-06-13 16:19:07,225] Epoch [2] Batch [380]: loss=0.47244\n",
      "[INFO 2022-06-13 16:19:07,786] Epoch [3] Batch [0]: loss=inf\n",
      "[INFO 2022-06-13 16:19:09,769] Epoch [3] Batch [20]: loss=0.46604\n",
      "[INFO 2022-06-13 16:19:11,430] Epoch [3] Batch [40]: loss=0.45303\n",
      "[INFO 2022-06-13 16:19:13,003] Epoch [3] Batch [60]: loss=0.44890\n",
      "[INFO 2022-06-13 16:19:14,658] Epoch [3] Batch [80]: loss=0.44717\n",
      "[INFO 2022-06-13 16:19:16,319] Epoch [3] Batch [100]: loss=0.44492\n",
      "[INFO 2022-06-13 16:19:17,897] Epoch [3] Batch [120]: loss=0.44298\n",
      "[INFO 2022-06-13 16:19:19,560] Epoch [3] Batch [140]: loss=0.44040\n",
      "[INFO 2022-06-13 16:19:21,225] Epoch [3] Batch [160]: loss=0.43841\n",
      "[INFO 2022-06-13 16:19:22,798] Epoch [3] Batch [180]: loss=0.43710\n",
      "[INFO 2022-06-13 16:19:24,456] Epoch [3] Batch [200]: loss=0.43598\n",
      "[INFO 2022-06-13 16:19:26,126] Epoch [3] Batch [220]: loss=0.43478\n",
      "[INFO 2022-06-13 16:19:27,712] Epoch [3] Batch [240]: loss=0.43371\n",
      "[INFO 2022-06-13 16:19:29,387] Epoch [3] Batch [260]: loss=0.43232\n",
      "[INFO 2022-06-13 16:19:31,076] Epoch [3] Batch [280]: loss=0.43082\n",
      "[INFO 2022-06-13 16:19:32,692] Epoch [3] Batch [300]: loss=0.42961\n",
      "[INFO 2022-06-13 16:19:34,357] Epoch [3] Batch [320]: loss=0.42804\n",
      "[INFO 2022-06-13 16:19:36,214] Epoch [3] Batch [340]: loss=0.42701\n",
      "[INFO 2022-06-13 16:19:38,059] Epoch [3] Batch [360]: loss=0.42572\n",
      "[INFO 2022-06-13 16:19:39,763] Epoch [3] Batch [380]: loss=0.42442\n",
      "[INFO 2022-06-13 16:19:40,309] Epoch [4] Batch [0]: loss=inf\n",
      "[INFO 2022-06-13 16:19:41,968] Epoch [4] Batch [20]: loss=0.38397\n",
      "[INFO 2022-06-13 16:19:43,616] Epoch [4] Batch [40]: loss=0.37258\n",
      "[INFO 2022-06-13 16:19:45,192] Epoch [4] Batch [60]: loss=0.36569\n",
      "[INFO 2022-06-13 16:19:46,840] Epoch [4] Batch [80]: loss=0.35918\n",
      "[INFO 2022-06-13 16:19:48,491] Epoch [4] Batch [100]: loss=0.35444\n",
      "[INFO 2022-06-13 16:19:50,066] Epoch [4] Batch [120]: loss=0.34902\n",
      "[INFO 2022-06-13 16:19:51,722] Epoch [4] Batch [140]: loss=0.34425\n",
      "[INFO 2022-06-13 16:19:53,384] Epoch [4] Batch [160]: loss=0.33936\n",
      "[INFO 2022-06-13 16:19:54,983] Epoch [4] Batch [180]: loss=0.33412\n",
      "[INFO 2022-06-13 16:19:56,855] Epoch [4] Batch [200]: loss=0.32884\n",
      "[INFO 2022-06-13 16:19:58,494] Epoch [4] Batch [220]: loss=0.32384\n",
      "[INFO 2022-06-13 16:20:00,082] Epoch [4] Batch [240]: loss=0.31903\n",
      "[INFO 2022-06-13 16:20:01,747] Epoch [4] Batch [260]: loss=0.31419\n",
      "[INFO 2022-06-13 16:20:03,413] Epoch [4] Batch [280]: loss=0.30950\n",
      "[INFO 2022-06-13 16:20:05,000] Epoch [4] Batch [300]: loss=0.30478\n",
      "[INFO 2022-06-13 16:20:06,668] Epoch [4] Batch [320]: loss=0.30016\n",
      "[INFO 2022-06-13 16:20:08,395] Epoch [4] Batch [340]: loss=0.29563\n",
      "[INFO 2022-06-13 16:20:10,135] Epoch [4] Batch [360]: loss=0.29082\n",
      "[INFO 2022-06-13 16:20:11,746] Epoch [4] Batch [380]: loss=0.28581\n",
      "[INFO 2022-06-13 16:20:12,389] Epoch [5] Batch [0]: loss=inf\n",
      "[INFO 2022-06-13 16:20:14,252] Epoch [5] Batch [20]: loss=0.16905\n",
      "[INFO 2022-06-13 16:20:15,915] Epoch [5] Batch [40]: loss=0.16172\n",
      "[INFO 2022-06-13 16:20:17,501] Epoch [5] Batch [60]: loss=0.15662\n",
      "[INFO 2022-06-13 16:20:19,174] Epoch [5] Batch [80]: loss=0.15229\n",
      "[INFO 2022-06-13 16:20:20,838] Epoch [5] Batch [100]: loss=0.14889\n",
      "[INFO 2022-06-13 16:20:22,427] Epoch [5] Batch [120]: loss=0.14623\n",
      "[INFO 2022-06-13 16:20:24,110] Epoch [5] Batch [140]: loss=0.14353\n",
      "[INFO 2022-06-13 16:20:25,823] Epoch [5] Batch [160]: loss=0.14111\n",
      "[INFO 2022-06-13 16:20:27,424] Epoch [5] Batch [180]: loss=0.13879\n",
      "[INFO 2022-06-13 16:20:29,084] Epoch [5] Batch [200]: loss=0.13671\n",
      "[INFO 2022-06-13 16:20:30,734] Epoch [5] Batch [220]: loss=0.13507\n",
      "[INFO 2022-06-13 16:20:32,331] Epoch [5] Batch [240]: loss=0.13299\n",
      "[INFO 2022-06-13 16:20:34,018] Epoch [5] Batch [260]: loss=0.13145\n",
      "[INFO 2022-06-13 16:20:35,679] Epoch [5] Batch [280]: loss=0.12957\n",
      "[INFO 2022-06-13 16:20:37,525] Epoch [5] Batch [300]: loss=0.12788\n",
      "[INFO 2022-06-13 16:20:39,114] Epoch [5] Batch [320]: loss=0.12608\n",
      "[INFO 2022-06-13 16:20:40,769] Epoch [5] Batch [340]: loss=0.12443\n",
      "[INFO 2022-06-13 16:20:42,482] Epoch [5] Batch [360]: loss=0.12283\n",
      "[INFO 2022-06-13 16:20:44,062] Epoch [5] Batch [380]: loss=0.12115\n",
      "[INFO 2022-06-13 16:20:44,615] Epoch [6] Batch [0]: loss=inf\n",
      "[INFO 2022-06-13 16:20:46,458] Epoch [6] Batch [20]: loss=0.08420\n",
      "[INFO 2022-06-13 16:20:48,431] Epoch [6] Batch [40]: loss=0.08130\n",
      "[INFO 2022-06-13 16:20:50,296] Epoch [6] Batch [60]: loss=0.08029\n",
      "[INFO 2022-06-13 16:20:51,883] Epoch [6] Batch [80]: loss=0.07968\n",
      "[INFO 2022-06-13 16:20:53,546] Epoch [6] Batch [100]: loss=0.07831\n",
      "[INFO 2022-06-13 16:20:55,209] Epoch [6] Batch [120]: loss=0.07749\n",
      "[INFO 2022-06-13 16:20:56,796] Epoch [6] Batch [140]: loss=0.07677\n",
      "[INFO 2022-06-13 16:20:58,464] Epoch [6] Batch [160]: loss=0.07583\n",
      "[INFO 2022-06-13 16:21:00,140] Epoch [6] Batch [180]: loss=0.07590\n",
      "[INFO 2022-06-13 16:21:01,716] Epoch [6] Batch [200]: loss=0.07525\n",
      "[INFO 2022-06-13 16:21:03,367] Epoch [6] Batch [220]: loss=0.07457\n",
      "[INFO 2022-06-13 16:21:05,012] Epoch [6] Batch [240]: loss=0.07360\n",
      "[INFO 2022-06-13 16:21:06,603] Epoch [6] Batch [260]: loss=0.07306\n",
      "[INFO 2022-06-13 16:21:08,349] Epoch [6] Batch [280]: loss=0.07279\n",
      "[INFO 2022-06-13 16:21:10,088] Epoch [6] Batch [300]: loss=0.07248\n",
      "[INFO 2022-06-13 16:21:11,774] Epoch [6] Batch [320]: loss=0.07184\n",
      "[INFO 2022-06-13 16:21:13,440] Epoch [6] Batch [340]: loss=0.07141\n",
      "[INFO 2022-06-13 16:21:15,116] Epoch [6] Batch [360]: loss=0.07109\n",
      "[INFO 2022-06-13 16:21:16,706] Epoch [6] Batch [380]: loss=0.07070\n"
     ]
    }
   ],
   "source": [
    "# define model here\n",
    "# model = CAT.model.IRTModel(**config)\n",
    "model = CAT.model.NCDModel(**config)\n",
    "# train model\n",
    "model.init_model(train_data)\n",
    "model.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.adaptest_save('../ckpt/irt.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "0caebb5e2435c3fd9cce368fa2d82b7ee4b348432df3fcc2b743002c5fe4d0d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
